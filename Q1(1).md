可以，我已经把你上传的 `model_df.csv` 读完了——**就 Q1 来说，你的数据基本已经够用了**。下面我直接把你要的三件事一次性“落地完成”：

1. **你还需不需要额外数据？需要的话怎么找？**
2. **哪些列是 (J)、哪些列是 (e_t)，以及每个 season 属于 percent / rank / rank_bottom2**
3. **最小可运行实现结构（数据预处理 → likelihood → MCMC → 结果图/表）**（我给你一份可以直接跑的 Python 代码）

---

## 1) Q1 还需要额外数据吗？（结论 + 建议）

### ✅ “必须”的额外数据：不需要

因为你这份 CSV 里已经包含 Q1 反演所需的核心信息：

* **赛季/周**：`season`, `week`
* **当周在赛集合 (A_{s,t})**：`active`
* **评委信息 (J)**：`judge_total`, `judge_share`, `judge_rank`
* **淘汰观测 (e_t)**：`is_eliminated_this_week`
* **规则分段**：`scheme`（而且每个 season 只有一种 scheme，非常好）

所以：**不找外部数据也能把 Q1 完整做出来**，并回答 certainty（用后验区间/后验预测概率）。

---

### ⭐ “可选但很加分”的额外数据（冲 O 奖建议做一个）

这些不是必须，但会让你们的 Q1 更像强队：**用外部“人气 proxy”作为信息先验**，减少不可识别性（多解）。

**推荐优先级（从易到难）：**

1. **Wikipedia Pageviews（强烈推荐）**

   * 用作赛季初每个选手的“基础人气”先验 (z_{i,1}) 的均值
   * 怎么找：用 Wikimedia 的 Pageviews API（可复现、免费、数据干净）
   * 需要你额外补一个：每季每周的大概播出日期（或至少 season 的起止月份）

2. **Google Trends（pytrends）**

   * 同样做先验或对后验做验证（你推出来的人气变化是否跟趋势一致）
   * 风险：地区、关键词歧义、接口不稳定（但写得好也很强）

3. **社媒粉丝数（Instagram/Twitter）**

   * 更偏手工/爬虫，复现性差一点（但可以作为 appendix）

> 如果你们时间紧：**Q1 先不加外部数据也完全 OK**；但要冲 O，我建议至少加 *Wikipedia pageviews* 这一项（最干净、最学术）。

---

## 2) 你的 CSV 里：列与数学符号的对应关系（已替你对齐）

### 2.1 你数据的表头（24 列）

我看到的列名是：
`season, celebrity_name, week, judge_count, judge_total, judge_mean, week_exists, active, judge_share, judge_rank, ballroom_partner, celebrity_industry, celebrity_homestate, celebrity_homecountry/region, celebrity_age_during_season, results, placement, is_withdrew_season, elim_n, is_eliminated_this_week, withdrew_event_week, withdrew_last_active_week, n_active, scheme`

---

### 2.2 哪些列用于 (J)（评委信息）

你有三种都能用，我建议这样用（最稳）：

* **百分比制（scheme = `percent`）**

  * 用 `judge_share` 作为 (\tilde J_{i,t})（它在每个 season-week 内求和≈1，非常适合）
  * `judge_total` 可留作检查或做 tie-break

* **排名制（scheme = `rank` 或 `rank_bottom2`）**

  * 用 `judge_rank` 作为 (R^J_{i,t})
  * `judge_total` 可用于你想做软排名/平滑时的备用

---

### 2.3 哪些列用于 (e_t)（淘汰观测）

* `is_eliminated_this_week == True` 的那些行，就是当周被淘汰的人（可能 1 个、2 个，甚至极少数 3 个）
* 注意：你数据里**确实存在 double / triple elimination**，要在 likelihood 里处理（我在代码里帮你做了“顺序无放回近似”）

---

### 2.4 哪些行是当周在赛集合 (A_{s,t})

* 直接用：`active == True`
  你这份数据很贴心：选手淘汰/退赛后仍保留行，但 `active=False` 且 `judge_total=0`，**建模时必须过滤掉**。

---

### 2.5 每个 season 属于 percent / rank / rank_bottom2（你已经在 `scheme` 给好了）

你数据里每个 season 的 scheme 是固定的，我读出来是：

* `rank`：**Season 1–2**
* `percent`：**Season 3–27**
* `rank_bottom2`：**Season 28–34**

这一步你不用再手工判断了，直接按 `scheme` 分支即可。

---

## 3) 最小可运行实现结构（我给你可直接跑的代码）

下面这份代码做了你要的“真正落地”：

* 读取你的 CSV
* 按 `season-week` 构造 (A_{s,t})、(J)、(e_t)
* 给出三种 scheme 的 likelihood：

  * `percent`：soft淘汰（分越低越容易淘汰）
  * `rank`：judge_rank + fan soft-rank
  * `rank_bottom2`：近似 bottom2 + judges 决策（无 save 数据时的合理近似）
* 跑一个可工作的 **Metropolis-Hastings**（最小版：先固定超参 sigma/beta/pi/tau，后续你再做敏感性扫描）
* 输出：

  * 每周每人 (F_{i,t}) 的后验均值 + 95% credible interval
  * 每周真实淘汰的后验预测概率（用来做“冷门/不确定性”）

> 你复制后直接改一行 `CSV_PATH` 就能跑。

```python
import numpy as np
import pandas as pd

# ======================
# 0) Load data
# ======================
CSV_PATH = "model_df.csv"   # 改成你的路径
df = pd.read_csv(CSV_PATH)

# ======================
# 1) Utility functions
# ======================
def softmax(x):
    x = np.asarray(x, dtype=float)
    m = np.max(x)
    ex = np.exp(x - m)
    return ex / np.sum(ex)

def soft_rank_desc(x, tau=0.1):
    """
    Soft rank for "larger is better" x.
    Returns ranks in [1, n], smaller rank = better.
    srank_i = 1 + sum_{k!=i} sigmoid((x_k - x_i)/tau)
    """
    x = np.asarray(x, dtype=float)
    tau = float(tau)
    assert tau > 0
    diff = (x.reshape(1, -1) - x.reshape(-1, 1)) / tau  # (x_k - x_i)/tau
    sig = 1 / (1 + np.exp(-diff))
    np.fill_diagonal(sig, 0.0)
    return 1.0 + sig.sum(axis=1)

# ======================
# 2) Build per-season structure
# ======================
def build_season(df, season):
    sub = df[df["season"] == season].copy()
    scheme = sub["scheme"].dropna().unique()[0]

    contestants = sub["celebrity_name"].dropna().unique().tolist()
    idx_map = {name: i for i, name in enumerate(contestants)}

    weeks = sorted(sub["week"].dropna().unique().tolist())
    week_data = {}

    for t in weeks:
        wk = sub[(sub["week"] == t) & (sub["active"] == True)].copy()
        if wk.empty:
            continue

        wk["idx"] = wk["celebrity_name"].map(idx_map)
        active_idx = wk["idx"].to_numpy()

        # J info
        judge_share = wk["judge_share"].to_numpy(dtype=float)  # percent scheme uses this
        judge_rank  = wk["judge_rank"].to_numpy(dtype=float)   # rank schemes use this

        # e_t observation: who eliminated this week (can be 0/1/2/3...)
        elim_idx_full = wk.loc[wk["is_eliminated_this_week"] == True, "idx"].to_numpy()

        week_data[t] = {
            "active_idx": active_idx,
            "judge_share": judge_share,
            "judge_rank": judge_rank,
            "elim_idx_full": elim_idx_full
        }

    weeks_kept = sorted(week_data.keys())
    return {
        "season": season,
        "scheme": scheme,
        "contestants": contestants,
        "week_data": week_data,
        "weeks": weeks_kept
    }

# ======================
# 3) Likelihood branches (three schemes)
# ======================
def elim_probs_percent(z_active, judge_share, beta=60.0):
    F = softmax(z_active)
    S = judge_share + F
    return softmax(-beta * S)  # low S -> high elim prob

def elim_probs_rank(z_active, judge_rank, beta=4.0, tau=0.05):
    F = softmax(z_active)
    RF = soft_rank_desc(F, tau=tau)   # smaller better
    B = judge_rank + RF               # bigger worse
    return softmax(beta * B)

def elim_probs_rank_bottom2(z_active, judge_rank, beta=4.0, tau=0.05):
    """
    A practical approximation for 'bottom2 + judges decision' when we don't observe 'saved':
    - bottom2 candidate set is likely the two worst by combined badness B
    - within bottom2, the one with worse judge_rank is more likely eliminated
    """
    F = softmax(z_active)
    RF = soft_rank_desc(F, tau=tau)
    B = judge_rank + RF

    # probability of being selected into bottom2 set (unordered) via sequential worst selection
    w = np.exp(beta * (B - np.max(B)))
    w = w / w.sum()
    n = len(B)

    p_set = np.zeros((n, n))
    for i in range(n):
        for j in range(i + 1, n):
            # P(c1=i,c2=j) + P(c1=j,c2=i)
            p_ij = w[i] * (w[j] / (1 - w[i]))
            p_ji = w[j] * (w[i] / (1 - w[j]))
            p_set[i, j] = p_set[j, i] = p_ij + p_ji

    # within pair {i,j}, eliminate more likely the worse judge_rank (bigger is worse)
    p_elim = np.zeros(n)
    for i in range(n):
        for j in range(n):
            if i == j:
                continue
            logits = beta * np.array([judge_rank[i], judge_rank[j]])
            m = np.max(logits)
            p_i = np.exp(logits[0] - m) / (np.exp(logits[0] - m) + np.exp(logits[1] - m))
            p_elim[i] += p_set[i, j] * p_i

    # normalize
    if p_elim.sum() > 0:
        p_elim = p_elim / p_elim.sum()
    else:
        p_elim = np.ones(n) / n
    return p_elim

def loglik_week(z_active, scheme, judge_share, judge_rank, elim_positions,
               beta=60.0, tau=0.05, pi=0.02):
    n = len(z_active)
    if scheme == "percent":
        p_rule = elim_probs_percent(z_active, judge_share, beta=beta)
    elif scheme == "rank":
        p_rule = elim_probs_rank(z_active, judge_rank, beta=beta, tau=tau)
    elif scheme == "rank_bottom2":
        p_rule = elim_probs_rank_bottom2(z_active, judge_rank, beta=beta, tau=tau)
    else:
        raise ValueError("Unknown scheme")

    # robust mixture for "cold upset / special week"
    p = (1 - pi) * p_rule + pi * (1.0 / n)

    # multiple eliminations: sequential without-replacement approximation
    remaining = np.ones(n, dtype=bool)
    ll = 0.0
    for pos in elim_positions:
        pr = p[remaining]
        pr = pr / pr.sum()
        rem_idx = np.where(remaining)[0]
        k = np.where(rem_idx == pos)[0][0]
        ll += np.log(max(pr[k], 1e-300))
        remaining[pos] = False
    return ll

# ======================
# 4) Random-walk prior on z (smoothness)
# ======================
def init_z_matrix(season_data, seed=0):
    weeks = season_data["weeks"]
    T = max(weeks)
    N = len(season_data["contestants"])
    z = np.zeros((T + 1, N))
    active_mask = np.zeros((T + 1, N), dtype=bool)

    for t in weeks:
        idx = season_data["week_data"][t]["active_idx"]
        active_mask[t, idx] = True

    rng = np.random.default_rng(seed)
    z[active_mask] = rng.normal(0, 0.1, size=active_mask.sum())
    return z, active_mask

def logprior_rw(z, active_mask, sigma=0.25):
    T = z.shape[0] - 1
    var = sigma ** 2
    lp = 0.0
    for t in range(2, T + 1):
        overlap = active_mask[t] & active_mask[t - 1]
        if overlap.any():
            d = z[t, overlap] - z[t - 1, overlap]
            lp += -0.5 * np.sum(d * d) / var - 0.5 * overlap.sum() * np.log(2 * np.pi * var)
    return lp

def loglik_season(z, season_data, beta, tau, pi):
    scheme = season_data["scheme"]
    ll = 0.0
    for t, wk in season_data["week_data"].items():
        elim_full = wk["elim_idx_full"]
        if elim_full.size == 0:
            continue  # no elimination observed

        active_idx = wk["active_idx"]
        pos_map = {idx: i for i, idx in enumerate(active_idx)}

        # deterministic order for multi-elim (practical, consistent)
        if scheme == "percent":
            order = sorted(elim_full.tolist(), key=lambda idx: wk["judge_share"][pos_map[idx]])
        else:
            order = sorted(elim_full.tolist(), key=lambda idx: wk["judge_rank"][pos_map[idx]], reverse=True)

        elim_positions = [pos_map[i] for i in order]
        z_active = z[t, active_idx]

        ll += loglik_week(
            z_active, scheme,
            wk["judge_share"], wk["judge_rank"],
            elim_positions,
            beta=beta, tau=tau, pi=pi
        )
    return ll

# ======================
# 5) Minimal Metropolis-Hastings (fixed hyperparams version)
# ======================
def run_mh_one_season(season_data,
                      n_iter=400, burn=150, thin=10,
                      sigma=0.25, beta=60.0, tau=0.05, pi=0.02,
                      step=0.10, seed=1):
    rng = np.random.default_rng(seed)
    z, active_mask = init_z_matrix(season_data, seed=seed)

    cur_lp = logprior_rw(z, active_mask, sigma=sigma)
    cur_ll = loglik_season(z, season_data, beta=beta, tau=tau, pi=pi)
    cur_post = cur_lp + cur_ll

    weeks = season_data["weeks"]
    samples = []
    acc = 0
    prop = 0

    for it in range(n_iter):
        for t in weeks:
            idx = np.where(active_mask[t])[0]
            if idx.size == 0:
                continue
            prop += 1

            z_prop = z.copy()
            z_prop[t, idx] = z[t, idx] + rng.normal(0, step, size=idx.size)

            lp_prop = logprior_rw(z_prop, active_mask, sigma=sigma)
            ll_prop = loglik_season(z_prop, season_data, beta=beta, tau=tau, pi=pi)
            post_prop = lp_prop + ll_prop

            if np.log(rng.random()) < (post_prop - cur_post):
                z = z_prop
                cur_lp, cur_ll, cur_post = lp_prop, ll_prop, post_prop
                acc += 1

        if it >= burn and ((it - burn) % thin == 0):
            samples.append(z.copy())

    return {
        "season": season_data["season"],
        "scheme": season_data["scheme"],
        "contestants": season_data["contestants"],
        "z_samples": samples,
        "accept_rate": acc / max(prop, 1)
    }

def summarize_posterior_F(z_samples, season_data):
    weeks = season_data["weeks"]
    contestants = season_data["contestants"]

    store = {(t, idx): [] for t in weeks for idx in season_data["week_data"][t]["active_idx"]}
    for z in z_samples:
        for t in weeks:
            wk = season_data["week_data"][t]
            active_idx = wk["active_idx"]
            F = softmax(z[t, active_idx])
            for pos, idx in enumerate(active_idx):
                store[(t, idx)].append(F[pos])

    rows = []
    for (t, idx), vals in store.items():
        vals = np.asarray(vals, dtype=float)
        rows.append({
            "season": season_data["season"],
            "week": t,
            "celebrity_name": contestants[idx],
            "F_mean": float(vals.mean()),
            "F_q025": float(np.quantile(vals, 0.025)),
            "F_q975": float(np.quantile(vals, 0.975)),
            "F_sd": float(vals.std(ddof=1)) if len(vals) > 1 else np.nan,
        })
    return pd.DataFrame(rows)

# ======================
# 6) Example: run one season
# ======================
# season_id = 3
# sd = build_season(df, season_id)
# # beta建议：percent用大一点（比如60），rank / rank_bottom2用小一点（比如4）
# beta = 60.0 if sd["scheme"] == "percent" else 4.0
# out = run_mh_one_season(sd, beta=beta, sigma=0.25, tau=0.05, pi=0.02)
# print("accept_rate =", out["accept_rate"])
# post = summarize_posterior_F(out["z_samples"], sd)
# print(post.head())
```

---

## 你下一步怎么用这份代码（不需要再问你问题）

### Step A：先把 Q1 跑通（建议先跑 1 个 percent season）

确认：

* accept_rate 合理（不是 0 也不是 1）
* 能输出 `F_mean / F_q025 / F_q975`
* 冷门周（拟合差）会表现为后验更分散（CI 更宽）或需要靠 `pi` 吸收

### Step B：再覆盖全部 season

你只要写个 loop：

* `for season in sorted(df.season.unique()):`
* `sd = build_season(df, season)`
* `beta = 60 if percent else 4`
* 跑完把 `posterior summary` concat 起来保存成一个 CSV
  （这就是你 Q1 的核心结果表）

### Step C：做 O 奖必须的 Q1 两张图

从 summary 里画：

1. **每周 top-k 选手的 (F_{i,t}) credible band**（均值 + 95%带）
2. **不确定性热力图**：CI 宽度 = (F_{q975}-F_{q025})

---

## 如果你问“我还需要找什么额外数据”——我的最终建议

* **Q1 不需要额外数据也能交出完整答案**（你现在的数据已满足）
* 但要冲 O：建议额外加 **Wikipedia pageviews**（给“基础人气先验”，并可做外部验证）

如果你愿意，我可以在你现有 CSV 的基础上，直接告诉你：
**Q1 结果表应该长什么样（列名、格式）**、以及你正文里“Certainty”的写法怎么把 **CI + 冷门周(pp_mean)** 一起讲得更像强队。
